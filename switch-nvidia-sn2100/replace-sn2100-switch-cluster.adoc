---
permalink: switch-nvidia-sn2100/replace-sn2100-switch-cluster.html 
sidebar: sidebar 
keywords: replacing, replace, defective, nvidia, switch, cluster, network, nondisruptive, procedure, ndu, replace a nvidia msn2100 cluster switch - nvidia SN2100 
summary: 'Der Austausch eines defekten NVIDIA SN2100-Switches in einem Cluster-Netzwerk ist ein unterbrechungsfreies Verfahren \(NDU\).' 
---
= Ersetzen Sie einen NVIDIA SN2100-Cluster-Switch
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Gehen Sie folgendermaßen vor, um einen defekten NVIDIA SN2100-Switch in einem Cluster-Netzwerk zu ersetzen. Dies ist ein NDU (Non Disruptive Procedure, NDU).



== Prüfen Sie die Anforderungen

.Bestehende Cluster- und Netzwerkinfrastruktur
Stellen Sie sicher, dass:

* Das vorhandene Cluster wird mit mindestens einem vollständig verbundenen Cluster-Switch als voll funktionsfähig geprüft.
* Alle Cluster-Ports sind aktiv.
* Alle logischen Cluster-Schnittstellen (LIFs) laufen und auf ihren Home-Ports.
* Das ONTAP `cluster ping-cluster -node node1` Der Befehl gibt an, dass grundlegende und größere Verbindungen als PMTU auf allen Pfaden erfolgreich sind.


.NVIDIA SN2100-Ersatzschalter
Stellen Sie sicher, dass:

* Die Konnektivität des Managementnetzwerks am Ersatz-Switch funktioniert.
* Der Konsolenzugriff auf den Ersatz-Switch erfolgt.
* Die Knotenverbindungen sind die Anschlüsse swp1 bis swp14.
* Alle Inter-Switch Link (ISL)-Ports sind an den Ports swp15 und swp16 deaktiviert.
* Die gewünschte Referenzkonfigurationsdatei (RCF) und der Bildschalter des Betriebssystems Cumulus werden auf den Switch geladen.
* Die anfängliche Anpassung des Schalters ist abgeschlossen.


Vergewissern Sie sich außerdem, dass alle Änderungen an früheren Standorten, wie STP, SNMP und SSH, auf den neuen Switch kopiert werden.


NOTE: Sie müssen den Befehl zum Migrieren einer Cluster-LIF von dem Node ausführen, auf dem die Cluster-LIF gehostet wird.



== Tauschen Sie den Schalter aus

.Zu den Beispielen
Die Beispiele in diesem Verfahren verwenden die folgende Nomenklatur für Switches und Knoten:

* Die Namen der vorhandenen NVIDIA SN2100-Switches lauten _sw1_ und _sw2_.
* Der Name des neuen NVIDIA SN2100 Switch lautet _nsw2_.
* Die Knotennamen sind _node1_ und _node2_.
* Die Cluster-Ports auf jedem Node lauten _e3a_ und _e3b_.
* Die Cluster LIF-Namen sind _node1_clus1_ und _node1_clus2_ für node1, und _node2_clus1_ und _node2_clus2_ für node2.
* Die Eingabeaufforderung für Änderungen an allen Cluster-Nodes lautet `cluster1::*>`
* Breakout-Ports haben das Format swp[Port]s[Breakout-Port 0-3]. Beispielsweise sind vier Breakout-Ports auf swp1 _swp1s0_, _swp1s1_, _swp1s2_ und _swp1s3_.


.Allgemeines zur Cluster-Netzwerktopologie
Dieses Verfahren basiert auf der folgenden Cluster-Netzwerktopologie:

.Beispieltopologie anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                        Ignore
                                                  Speed(Mbps)  Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper   Status   Status
--------- ------------ ---------------- ---- ---- ------------ -------- ------
e3a       Cluster      Cluster          up   9000  auto/100000 healthy  false
e3b       Cluster      Cluster          up   9000  auto/100000 healthy  false

Node: node2
                                                                        Ignore
                                                  Speed(Mbps)  Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper   Status   Status
--------- ------------ ---------------- ---- ---- ------------ -------- ------
e3a       Cluster      Cluster          up   9000  auto/100000 healthy  false
e3b       Cluster      Cluster          up   9000  auto/100000 healthy  false


cluster1::*> *network interface show -vserver Cluster*

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
Cluster
            node1_clus1  up/up    169.254.209.69/16  node1         e3a     true
            node1_clus2  up/up    169.254.49.125/16  node1         e3b     true
            node2_clus1  up/up    169.254.47.194/16  node2         e3a     true
            node2_clus2  up/up    169.254.19.183/16  node2         e3b     true


cluster1::*> *network device-discovery show -protocol lldp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface     Platform
----------- ------ ------------------------- ------------  ----------------
node1      /lldp
            e3a    sw1 (b8:ce:f6:19:1a:7e)   swp3          -
            e3b    sw2 (b8:ce:f6:19:1b:96)   swp3          -
node2      /lldp
            e3a    sw1 (b8:ce:f6:19:1a:7e)   swp4          -
            e3b    sw2 (b8:ce:f6:19:1b:96)   swp4          -
----
+

[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost         RemotePort
---------  -----  ----------  -----------------  -----------
swp3       100G   Trunk/L2    sw2                e3a
swp4       100G   Trunk/L2    sw2                e3a
swp15      100G   BondMember  sw2                swp15
swp16      100G   BondMember  sw2                swp16


cumulus@sw2:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost         RemotePort
---------  -----  ----------  -----------------  -----------
swp3       100G   Trunk/L2    sw1                e3b
swp4       100G   Trunk/L2    sw1                e3b
swp15      100G   BondMember  sw1                swp15
swp16      100G   BondMember  sw1                swp16
----
====


=== Schritt 1: Vorbereitung auf den Austausch

. Wenn AutoSupport in diesem Cluster aktiviert ist, unterdrücken Sie die automatische Erstellung eines Falls durch Aufrufen einer AutoSupport Meldung:
+
`system node autosupport invoke -node * -type all -message MAINT=xh`

+
Wobei _x_ die Dauer des Wartungsfensters in Stunden ist.

. Ändern Sie die Berechtigungsebene in Erweitert, und geben Sie *y* ein, wenn Sie dazu aufgefordert werden, fortzufahren:
+
`set -privilege advanced`

+
Die erweiterte Eingabeaufforderung (*>) wird angezeigt.

. Installieren Sie das entsprechende RCF und das entsprechende Image auf dem Switch, nsw2, und treffen Sie die erforderlichen Standortvorbereitungen.
+
Überprüfen, laden und installieren Sie gegebenenfalls die entsprechenden Versionen der RCF- und Cumulus-Software für den neuen Switch.

+
.. Sie können die entsprechende Cumulus-Software für Ihre Cluster-Switches von der Seite _NVIDIA Support_ herunterladen. Folgen Sie den Schritten auf der Download-Seite, um das Cumulus Linux für die Version der ONTAP Software, die Sie installieren, herunterzuladen.
.. Das entsprechende RCF ist im erhältlich link:https://mysupport.netapp.com/site/products/all/details/nvidia-cluster-storage-switch/downloads-tab["_NVIDIA Cluster und Storage Switches_"^] Seite. Befolgen Sie die Schritte auf der Download-Seite, um den korrekten RCF für die Version der von Ihnen installierenden ONTAP-Software herunterzuladen.






=== Schritt: Ports und Verkabelung konfigurieren

. Melden Sie sich beim neuen Switch nsw2 als admin an und fahren Sie alle Ports herunter, die mit den Node-Cluster-Schnittstellen verbunden werden (Ports swp1 bis swp14).
+
Die LIFs auf den Cluster-Nodes sollten für jeden Node bereits ein Failover auf den anderen Cluster-Port durchgeführt haben.

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@nsw2:~$ *net add interface swp1s0-3, swp2s0-3, swp3-14 link down*
cumulus@nsw2:~$ *net pending*
cumulus@nsw2:~$ *net commit*
----
====
. Deaktivieren Sie die automatische Zurücksetzung auf den Cluster-LIFs:
+
`network interface modify -vserver Cluster -lif * -auto-revert false`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface modify -vserver Cluster -lif * -auto-revert false*

Warning: Disabling the auto-revert feature of the cluster logical interface may effect the availability of your cluster network. Are you sure you want to continue? {y|n}: *y*
----
====
. Vergewissern Sie sich, dass für alle Cluster-LIFs die automatische Zurücksetzung aktiviert ist:
+
`net interface show -vserver Cluster -fields auto-revert`

. Schließen Sie die ISL-Ports swp15 und swp16 am SN2100-Switch sw1 ab.
+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net add interface swp15-16 link down*
cumulus@sw1:~$ *net pending*
cumulus@sw1:~$ *net commit*
----
====
. Entfernen Sie alle Kabel vom SN2100 sw1-Switch, und verbinden Sie sie dann mit den gleichen Ports am SN2100 nsw2-Switch.
. Die ISL-Ports swp15 und swp16 zwischen den Switches sw1 und nsw2.
+
.Beispiel anzeigen
[%collapsible]
====
Die folgenden Befehle ermöglichen ISL-Ports swp15 und swp16 auf Switch sw1:

[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net del interface swp15-16 link down*
cumulus@sw1:~$ *net pending*
cumulus@sw1:~$ *net commit*
----
Das folgende Beispiel zeigt, dass die ISL-Ports auf Switch sw1 aufstehen:

[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP           Summary
-----  -----------  ----  -----  ----------  -------------- ----------------------
...
...
UP     swp15        100G  9216   BondMember  nsw2 (swp15)   Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  nsw2 (swp16)   Master: cluster_isl(UP)
----
+ das folgende Beispiel zeigt, dass die ISL-Ports auf Switch nsw2 sind:

+

[listing, subs="+quotes"]
----
cumulus@nsw2:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP           Summary
-----  -----------  ----  -----  ----------  -------------  -----------------------
...
...
UP     swp15        100G  9216   BondMember  sw1 (swp15)    Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  sw1 (swp16)    Master: cluster_isl(UP)
----
====
. Überprüfen Sie diesen Port `e3b` Ist auf allen Knoten aktiv:
+
`network port show -ipspace Cluster`

+
.Beispiel anzeigen
[%collapsible]
====
Die Ausgabe sollte wie folgt aussehen:

[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                         Ignore
                                                   Speed(Mbps)  Health   Health
Port      IPspace      Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ------------ ---------------- ---- ----- ------------ -------- -------
e3a       Cluster      Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster      Cluster          up   9000  auto/100000  healthy  false


Node: node2
                                                                         Ignore
                                                   Speed(Mbps) Health    Health
Port      IPspace      Broadcast Domain Link MTU   Admin/Oper  Status    Status
--------- ------------ ---------------- ---- ----- ----------- --------- -------
e3a       Cluster      Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster      Cluster          up   9000  auto/100000  healthy  false
----
====
. Die Cluster-Ports auf jedem Node sind nun aus Sicht der Nodes mit Cluster-Switches auf die folgende Weise verbunden:
+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol lldp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface     Platform
----------- ------ ------------------------- ------------  ----------------
node1      /lldp
            e3a    sw1  (b8:ce:f6:19:1a:7e)   swp3          -
            e3b    nsw2 (b8:ce:f6:19:1b:b6)   swp3          -
node2      /lldp
            e3a    sw1  (b8:ce:f6:19:1a:7e)   swp4          -
            e3b    nsw2 (b8:ce:f6:19:1b:b6)   swp4          -
----
====
. Vergewissern Sie sich, dass alle Node-Cluster-Ports aktiv sind:
+
`net show interface`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@nsw2:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP              Summary
-----  -----------  ----  -----  ----------  ----------------- ----------------------
...
...
UP     swp3         100G  9216   Trunk/L2                      Master: bridge(UP)
UP     swp4         100G  9216   Trunk/L2                      Master: bridge(UP)
UP     swp15        100G  9216   BondMember  sw1 (swp15)       Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  sw1 (swp16)       Master: cluster_isl(UP)
----
====
. Vergewissern Sie sich, dass beide Knoten jeweils eine Verbindung zu jedem Switch haben:
+
`net show lldp`

+
.Beispiel anzeigen
[%collapsible]
====
Das folgende Beispiel zeigt die entsprechenden Ergebnisse für beide Switches:

[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost         RemotePort
---------  -----  ----------  -----------------  -----------
swp3       100G   Trunk/L2    node1              e3a
swp4       100G   Trunk/L2    node2              e3a
swp15      100G   BondMember  nsw2               swp15
swp16      100G   BondMember  nsw2               swp16


cumulus@nsw2:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost         RemotePort
---------  -----  ----------  -----------------  -----------
swp3       100G   Trunk/L2    node1                e3b
swp4       100G   Trunk/L2    node2                e3b
swp15      100G   BondMember  sw1                swp15
swp16      100G   BondMember  sw1                swp16
----
====
. Aktivieren Sie die automatische Zurücksetzung auf den Cluster-LIFs:
+
`cluster1::*> network interface modify -vserver Cluster -lif * -auto-revert true`

. Bringen Sie auf Switch nsw2 die Ports an, die mit den Netzwerkports der Knoten verbunden sind.
+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@nsw2:~$ *net del interface swp1-14 link down*
cumulus@nsw2:~$ *net pending*
cumulus@nsw2:~$ *net commit*
----
====
. Zeigen Sie Informationen über die Nodes in einem Cluster an:
+
`cluster show`

+
.Beispiel anzeigen
[%collapsible]
====
Dieses Beispiel zeigt, dass der Zustand des Node für Node 1 und node2 in diesem Cluster „true“ lautet:

[listing, subs="+quotes"]
----
cluster1::*> *cluster show*

Node          Health  Eligibility
------------- ------- ------------
node1         true    true
node2         true    true
----
====
. Vergewissern Sie sich, dass alle physischen Cluster-Ports aktiv sind:
+
`network port show ipspace Cluster`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node node1                                                               Ignore
                                                    Speed(Mbps) Health   Health
Port      IPspace     Broadcast Domain  Link  MTU   Admin/Oper  Status   Status
--------- ----------- ----------------- ----- ----- ----------- -------- ------
e3a       Cluster     Cluster           up    9000  auto/10000  healthy  false
e3b       Cluster     Cluster           up    9000  auto/10000  healthy  false

Node: node2
                                                                         Ignore
                                                    Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link  MTU   Admin/Oper  Status   Status
--------- ------------ ---------------- ----- ----- ----------- -------- ------
e3a       Cluster      Cluster          up    9000  auto/10000  healthy  false
e3b       Cluster      Cluster          up    9000  auto/10000  healthy  false
----
====




=== Schritt 3: Führen Sie den Vorgang durch

. Vergewissern Sie sich, dass das Cluster-Netzwerk ordnungsgemäß funktioniert.
+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost      RemotePort
---------  -----  ----------  --------------  -----------
swp3       100G   Trunk/L2    node1           e3a
swp4       100G   Trunk/L2    node2           e3a
swp15      100G   BondMember  nsw2            swp15
swp16      100G   BondMember  nsw2            swp16
----
====
. Erstellen Sie ein Passwort für die Protokollerfassungsfunktion der Ethernet-Switch-Statusüberwachung:
+
`system switch ethernet log setup-password`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system switch ethernet log setup-password*
Enter the switch name: *<return>*
The switch name entered is not recognized.
Choose from the following list:
*cs1*
*cs2*

cluster1::*> *system switch ethernet log setup-password*

Enter the switch name: *cs1*
Would you like to specify a user other than admin for log collection? {y|n}: *n*

Enter the password: *<enter switch password>*
Enter the password again: *<enter switch password>*

cluster1::*> *system switch ethernet log setup-password*

Enter the switch name: *cs2*
Would you like to specify a user other than admin for log collection? {y|n}: *n*

Enter the password: *<enter switch password>*
Enter the password again: *<enter switch password>*
----
====
. Aktivieren Sie die Funktion zur Statusüberwachung des Ethernet-Switches.
+
`system switch ethernet log modify -device _<switch-name>_ -log-request true`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system switch ethernet log modify -device cs1 -log-request true*

Do you want to modify the cluster switch log collection configuration? {y|n}: [n] *y*

Enabling cluster switch log collection.

cluster1::*> *system switch ethernet log modify -device cs2 -log-request true*

Do you want to modify the cluster switch log collection configuration? {y|n}: [n] *y*

Enabling cluster switch log collection.
----
====
+
Warten Sie 10 Minuten, und überprüfen Sie dann, ob die Protokollsammlung abgeschlossen ist:

+
`system switch ethernet log show`

+
.Beispiel anzeigen
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> system switch ethernet log show
Log Collection Enabled: true

Index  Switch                       Log Timestamp        Status
------ ---------------------------- -------------------  ---------    
1      cs1 (b8:ce:f6:19:1b:42)      4/29/2022 03:05:25   complete   
2      cs2 (b8:ce:f6:19:1b:96)      4/29/2022 03:07:42   complete
----
====
+

CAUTION: Wenn einer dieser Befehle einen Fehler zurückgibt oder die Protokollsammlung nicht abgeschlossen ist, wenden Sie sich an den NetApp Support.

. Ändern Sie die Berechtigungsebene zurück in den Administrator:
+
`set -privilege admin`

. Wenn Sie die automatische Case-Erstellung unterdrückt haben, aktivieren Sie es erneut, indem Sie eine AutoSupport Meldung aufrufen:
+
`system node autosupport invoke -node * -type all -message MAINT=END`


